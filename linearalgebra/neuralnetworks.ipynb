{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b0096b-3571-4ac2-95e2-b0237a4fd065",
   "metadata": {},
   "source": [
    "# 3Blue1Brown on neural networks\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "\n",
    "\n",
    "- [Grant Sanderson's first video of seven: The Neural Network series](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "- [Steve Brunton's Machine Learning Primer](https://www.youtube.com/watch?v=Vx2DpMgplEM)\n",
    "- [Michael Nielsen's book on Deep Learning and Neural Nets](http://neuralnetworksanddeeplearning.com/)\n",
    "    - [Michael Nielsen's MNIST OCR walkthrough](https://github.com/mnielsen/neural-networks-and-deep-learning)\n",
    "- [Gil Strang's course reference at MIT](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/)\n",
    "    - [Gil Strang: First in a CNN lecture series](https://www.youtube.com/watch?v=sx00s7nYmRM&pp=ygUaZ2lsIHN0cmFuZyBuZXVyYWwgbmV0d29ya3M%3D)\n",
    "- [Chris Olah's blog](http://colah.github.io/)\n",
    "- [Andrej Karpathy: Build GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=0s)\n",
    "- [Articles at *distill*](https://distill.pub/)\n",
    "- [Samson Zhang: Building an MNIST neural net using NumPy](https://www.youtube.com/watch?v=w8yWXqWQYmU)\n",
    "    - Avoids higher abstractions like PyTorch\n",
    "- [Historical background on machine learning / neural nets](https://www.youtube.com/watch?v=1il-s4mgNdI&t=0s)\n",
    "\n",
    "\n",
    "## Fundamentals\n",
    "\n",
    "\n",
    "**Machine Learning** uses data to somehow determine how a model behaves.\n",
    "\n",
    "\n",
    "The *lesson* from the past couple decades: Scale alone gives us huge improvement\n",
    "in model behavior.\n",
    "\n",
    "\n",
    "## Video 1: But what is a neural network?\n",
    "\n",
    "\n",
    "- The 28 x 28 pixel grid gives values called *activations*\n",
    "    - This is stacked to give the activation layer\n",
    "    - We end up not caring how the pixels are sorted into this layer\n",
    "        - This mapping intentionally does not try to retain the spatial information of the grid\n",
    "- The activation value for each pixel is real on [0, 1]\n",
    "- The second layer happens to have 16 neurons; 784 x 16 weights\n",
    "- There are also 16 bias values, one for each layer-2 neuron\n",
    "    - Bias acts like a threshold: It sets a bar to clear\n",
    "- The result is of the weighted sum plus bias is unconstrained...\n",
    "    - ...but we are interested in something constrained to be on (say) [0, 1]\n",
    "    - This is precisely analogous to a compressor in audio signal processing\n",
    "    - This compression was originally done with a sigmoid function, as in:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "More recently this has becomea a Rectifier Linear Unit funciton: $ReLU(a) = max(0, a)$.\n",
    "Pronounced \"Ray - Loo\".\n",
    "Gil Strand refers to this in one of his lectures. I will continue to indicate compression\n",
    "as $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff469b-a5b9-409c-99ed-338969d94a33",
   "metadata": {},
   "source": [
    "- sum of weights = 784 * 16 + 16 * 16 + 16 * 10\n",
    "- sum of biases = 16 + 16 + 10\n",
    "\n",
    "\n",
    "13k total parameters\n",
    "\n",
    "\n",
    "Exhortation: Dig into ***why?*** Challenge your assumptions.\n",
    "\n",
    "\n",
    "- Activations are a $1 \\times n$ column vector $A$\n",
    "- Weights are an $m \\times n$ matrix $W$\n",
    "- Bias is a $1 \\times m$ column vector\n",
    "- Second layer of neurons is a $1 \\times m$ column vector $N$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\n",
    "N = \\sigma( W \\cdot A + B )\n",
    "}\n",
    "\\end{align}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7909d52-d14c-47f7-843c-3898b0fc0561",
   "metadata": {},
   "source": [
    "## Video 2: Gradient descent and how neural networks learn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Introduces a training dataset: With answers!\n",
    "- MNIST database is freely available\n",
    "- Now we start in on *the calculus exercise*\n",
    "- Initial: Random values\n",
    "\n",
    "\n",
    "Now we are introduced to a cost function: \n",
    "\n",
    "\n",
    "- Go through the (presumably wrong) values in the output vector $a_i$.\n",
    "- For each: Difference the value from the correct value $0, 0, 0, 0, 1, 0, 0, 0, 0, 0$\n",
    "- Square this\n",
    "- Add them up\n",
    "- Higher cost value: The worse this set of weights performed\n",
    "- This is a result for but one example\n",
    "    - This produces a single number from the combination of 784 pixel values and 13k weights and biases\n",
    "\n",
    "\n",
    "Consider the average cost over the entire training set.\n",
    "\n",
    "\n",
    "Now drop into single-variable calculus thinking: Use the local slope to determine a step to take\n",
    "in searching for a minimum. Steeper slope: Bigger step. Small slope: Small step (avoid overshoot).\n",
    "\n",
    "\n",
    "Now move to cost as a surface above two variables (up from one). Now we are taking gradients.\n",
    "\n",
    "\n",
    "So what is moving to calculate the local surface gradient? Those two variables; suppose they are\n",
    "the first two weights $w_1$ and $w_2$. Now we have the steepest direction; and we use the negative\n",
    "of that (as it points *upward*) to descend to some minimum. But of course there is the danger \n",
    "that it is a *local* minimum.\n",
    "\n",
    "\n",
    "And now of course go from 2 dimensions to 13,002 dimensions of input.\n",
    "\n",
    "\n",
    "$- \\nabla C(\\vec{w})$\n",
    "\n",
    "\n",
    "Gradient calculation in this context is called *back propagation*: Next video.\n",
    "\n",
    "\n",
    "\n",
    "All we mean when we say a network is learning is that it is minimizing its cost function. \n",
    "\n",
    "\n",
    "\n",
    "So let's keep in mind: \n",
    "\n",
    "\n",
    "- We have multiple layers of neurons, weights, biases to start with\n",
    "- These represent a progression from 784 inputs to one result\n",
    "- And we have test data including *correct* answers\n",
    "- So we can determine cost function values across the entire test dataset\n",
    "- And this cost function $C$ has a mean value\n",
    "- And somehow we can calculate the gradient of $C$: A 13,000 element vector\n",
    "    - Whose elements indicate changes to apply; by both sign and magnitude\n",
    "    - ...so that we can start over with new weights; and iterate\n",
    "\n",
    "\n",
    "Then there is **the test**: Score a version of the network on data it has never seen before.\n",
    "\n",
    "\n",
    "\n",
    "### Does the network behave understandably?\n",
    "\n",
    "\n",
    "\n",
    "- No. \n",
    "- The weights from the 784 layer to the 16 layer (when viewed as 28 x 28 images) are just random-looking...\n",
    "- ...all 16 of them...\n",
    "- ...and will classify random junk as a particular digit with high confidence\n",
    "- ...and has no mechanism to actually *draw* an archetype 3\n",
    "\n",
    "\n",
    "Grant observes that there is no mechanism in the system for uncertainty. The result vectors\n",
    "for example are always certain: 9 zeros and a single 1.\n",
    "\n",
    "\n",
    "The video concludes with remarks on \"memorizing the dataset\" by means of all these parameters; \n",
    "what I suspect is called over-fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be14871-1fd6-4f33-a3de-c3a2d4159daf",
   "metadata": {},
   "source": [
    "## Video 3: Backpropagation: Plausible story\n",
    "\n",
    "\n",
    "- Backpropagation as a compelling story without notation or calculus\n",
    "- Consider weights and activations in the penultimate layer\n",
    "    - Proceeding from the weighted sum: Converse concept:\n",
    "        - \"Modify Weights where Activations are high for impact!\"\n",
    "        - \"Modify Activations where Weights are high for impact!\"\n",
    "    - Complexity concept:\n",
    "        - We are considering 10 (not just the correct +1 neuron) end activations\n",
    "        - Multiplexing means individual cases are given a vote, not final say\n",
    "\n",
    "\n",
    "We want to find a minimum, hence gradient descent. \n",
    "\n",
    "\n",
    "## Video 4: Backpropagation: Calculus basis\n",
    "\n",
    "\n",
    "\"What is SGD?\"\n",
    "\n",
    "\n",
    "- Backpropagation as calculus\n",
    "- Principle idea is multi-variate calculus chain rule\n",
    "- A given end-neuron (e.g. answer = 3) is impacted by an activation (from the prior neuron layer), a weight and a bias\n",
    "- The idea is to get the gradient of the cost function in terms of chain rule partial derivatives\n",
    "- Back-prop refers to chaining backwards up the network until you arrive at the stimuls activation layer. These can not be modified.\n",
    "- Also we can think in terms of modifying other feeder neuron activation layers...\n",
    "    - ...but this is actually expressed in terms of -- in turn -- that neuron's inbound weight and bias\n",
    "- The other dial here\n",
    "    - We calculate the cost function gradient for one input case\n",
    "    - ...but in fact we do some sort of average over many training inputs\n",
    "    - ...but not every single one as this becomes computationally prohibitive\n",
    "    - ...so we resort to random samples creating cohorts\n",
    "    - ...hence stochastic\n",
    "    - ...hence stochastic gradient descent\n",
    "    - ...hence SGD\n",
    " \n",
    "\n",
    "> Note: When you use backpropagation you are in the Deep Learning space.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124c4b-c06f-4d20-b093-0e0a21b1928a",
   "metadata": {},
   "source": [
    "## Video 5: GPT\n",
    "\n",
    "\n",
    "* Transformers are the next key idea\n",
    "    * Consist of alternating **Attention Blocks** and **Perceptron Blocks**\n",
    "    * Predict the next word by generating a pdf over a set of words\n",
    "* First we need encoding as tokens (words or fragments)\n",
    "* Then we need embedding: Each token is assigned a vector value\n",
    "    * The vector space is \"defined\" by the model\n",
    "    * Similar words (I use words instead of tokens as roughly equivalent)...\n",
    "        * ...wind up with roughly aligned vector values\n",
    "    * The processing step is now to iterate through Attention and Perceptron blocks\n",
    "        * The Attention block allows the series of vectors to interact with one another\n",
    "            * ...hence the vector elements are modified\n",
    "        * The Perceptron blocks operate on the vectors in parallel\n",
    "            * ...so no interactivity in this step\n",
    "            * \"Multi-layer Perceptron\" (MLP) or equivalently \"Feed Forward Layer\"\n",
    "            * What this does is, for now, a mystery\n",
    "                * ...but it is certainly more linear algebra\n",
    "        * There is also some normalization going on, in passing\n",
    "     \n",
    "The end result is understood as the now-thoroughly-modified last vector in\n",
    "the token sequence. And this modified vector is then used to generate the pdf,\n",
    "a distribution of probabilities for a set of possible \"next words\".\n",
    "\n",
    "\n",
    "Now so far this is \"next word prediction\". To go to Chatbot we have\n",
    "\n",
    "- A System prompt (\"The Chatbot is a helpful yadda yadda\")\n",
    "- An initial User prompt\n",
    "\n",
    "Now in some sense the model is \"Predicting what that helpful assistant would say...\",\n",
    "an odd bit of rhetorical indirection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c86ba6-6584-407b-8b73-484e727adbb7",
   "metadata": {},
   "source": [
    "The model uses (100B+) weights organized into matrices, in turn \n",
    "divided up into eight categories.\n",
    "\n",
    "\n",
    "Pre-processing: Tokenization\n",
    "\n",
    "Categories\n",
    "\n",
    "\n",
    "- embedding: One entry for each token/word\n",
    "    - This translates our English query into a sequence of vectors\n",
    "    - GPT 3 uses 12,288 dimensions for these vectors\n",
    "    - Vector alignment (embedding similarity): Inner (dot) product\n",
    "    - 600 million weights in total just for token > embedded vector\n",
    "    - How many tokens are allowed?\n",
    "        - GPT 3: 2000 tokens, the **Context Size**\n",
    "    - Look-up (embedding) matrix is $W_e$\n",
    "- Together Query, Key and Value comprise one **Head** of attention\n",
    "    - query\n",
    "        - Lower-dimensional space: Asks about word modification of meaning\n",
    "    - key\n",
    "        - Lower-dimensional space: Answers query for relevance\n",
    "    - value\n",
    "        - Generates a vector based on a relevant modifier\n",
    "        - This is added to the embedding of the modified word\n",
    "        - ...and this changes its location in the embedding space\n",
    "    - Words that follow are not allowed to modify words that precede\n",
    "- output\n",
    "- up-projection\n",
    "- down-projection\n",
    "- unembedding\n",
    "    - Simple idea: Works on the very last highly-modified embedding vector\n",
    "        - So we will be looking at $12k$ values in just that last vector\n",
    "    - Reality is more complicated\n",
    "    - Call the un-embedding matrix $W_u$\n",
    " \n",
    "**Softmax** is an operation on a vector of arbitrary values that maps it to a \n",
    "legal pdf: Sum is 1 and individual values are on [0, 1]. This gets us to the\n",
    "idea of *Temperature* which in this context is a user-chosen value that acts\n",
    "as a \"creativity dial\". Higher temperature means the GPT responses will stretch\n",
    "further down into the pdf (lower probabilities) to select that next word. \n",
    "\n",
    "If the vector $\\vec{x} = \\{ x_0, \\dots , x_{N-1} \\}$ then the softmax-modified\n",
    "value of $x_n$ (given a temperature $T$) is given by:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\\Huge {x'}_{n} = {e^{\\frac{x_n}{T}}}/{\\sum_{i=0}^{N-1}{e^{\\frac{x_i}{T}}}}\n",
    "}\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "Now we have an idea of softmax, of embedding, general use of \n",
    "matrix $\\times$ vector in this process, and inner product for similarity...\n",
    "so we're invited to the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee15be0-bc64-4fd9-96ec-adbd4fa9f714",
   "metadata": {},
   "source": [
    "## Video 6 Transformers\n",
    "\n",
    "\n",
    "\"Attention is all you need\" (2017)\n",
    "\n",
    "\n",
    "Embedding: Direction corresponds to semantic meaning\n",
    "\n",
    "\n",
    "A mole is a mole is a mole (lookup table).\n",
    "\n",
    "\n",
    "Attention block says \"That's not true!\"\n",
    "\n",
    "\n",
    "New development: The embedding vector also encodes the position of the token in\n",
    "the source: Location context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eda18d-71d5-4319-b714-dbfc2c701269",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e65c05-7933-4527-b752-3cbc614c036a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53ae9316-6568-4f17-a9c9-86e3fbf01bee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
