{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'Sixes' Dice Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sixes and fives](../images/probability/sixes_and_fives.jpg)\n",
    "\n",
    "\n",
    "A pair of sixes and a pair of fives. \n",
    "\n",
    "\n",
    "\n",
    "## The game\n",
    "\n",
    "This notebook poses a question in the form of a game; and then it works along to an answer.\n",
    "It takes some doing; so the interested reader is invited to treat this as a journey!\n",
    "\n",
    "\n",
    "The game: Begin with a handful of (six-sided) dice and roll them all at once. Remove\n",
    "all those dice that came up 6. Repeat the process with the remaining dice. The object is\n",
    "to remove all the dice in as few such rolls as possible. Truly a game of skill...\n",
    "\n",
    "\n",
    "The question is: If we were to play this game many times with $N$ dice, how many rolls do we expect \n",
    "the game to take to finish?\n",
    "\n",
    "\n",
    "To avoid confusion: Unless otherwise noted, the word *dice* in this notebook refers to one or more \n",
    "cubes with faces marked sequentially one through six. When a dice or several dice are rolled: their \n",
    "upper face is the result; a random number from one to six. All dice are fair: Each of the six \n",
    "faces has a $1/6$ probability of being that upper face. One exception below: I use an eight-sided\n",
    "or octahedral dice. \n",
    "\n",
    "\n",
    "\n",
    "## Probability Ideas\n",
    "\n",
    "\n",
    "Just like on any journey we'll stock up on some supplies -- ideas -- before setting out. There are\n",
    "about half a dozen ideas that will prove useful. And for the record I sometimes make up my own\n",
    "terminology... so be ready to learn better terminology elsewhere!\n",
    "\n",
    "\n",
    "### Idea Zero: What is a probability (as a number)?\n",
    "\n",
    "\n",
    "Probabilities are traditionally numbers from $0$ to $1$ inclusive. This applies to some sort of event or \n",
    "experiment that has an observable outcome. If I roll a die (the experiment) I will arrive at an outcome\n",
    "like \"I rolled a 3\". Let's agree to designate a probability as $p$. In the event\n",
    "that $p \\; = \\; 1$ we can say that whatever is the corresponding outcome is a *sure thing*. If $p \\; = \\; 0$\n",
    "then that outcome has *no chance*.\n",
    "\n",
    "\n",
    "### Idea One: What is a complementary probability?\n",
    "\n",
    "\n",
    "If the probability of an outcome $A$ is $p$ then we can call that $p(A)$. \n",
    "The probability of that outcome *not happening* is $1-p(A)$. We use a tilde $~$ \n",
    "before the $A$ to indicate *'not A'* or *'A does not happen'*. So $p(~A)\\; = \\; 1 \\; - \\; p(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications of Ideas 0 and 1\n",
    "\n",
    "\n",
    "A calculated probability that is less than $0$ or greater than $1$ is an error. One could experiment \n",
    "with stretching the definition of a probability but for this discussion: That's the rule.\n",
    "\n",
    "\n",
    "A probability $p$ for some outcome means that -- were one to perform the experiment $n$ times -- \n",
    "one would expect $p \\cdot n$ such outcomes. This perspective concerns the *frequency* of the outcome.\n",
    "People who talk about probability in this way are called\n",
    "*frequentists*. This is the traditional approach to getting started with probability.\n",
    "\n",
    "\n",
    "Example: For ''t'' throws, the probability of never once rolling a 6 is $(5/6)^t$. P(rolling 6 at least once) is therefore $1 - (5/6)^t$ and this is much simpler than the equivalent expression derived by \"counting ways\" per Idea 2 (next).\n",
    "\n",
    "\n",
    "$$P \\;(A) = 1 - P \\;(!A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 2: The Product of Independent Probabilities: 'AND' Questions\n",
    "\n",
    "\n",
    "The ideas of 'independent' versus 'dependent' events are confusing in probability theory (to me), \n",
    "in comparison with how they are used commonly. Let's look at two independent events for a moment.\n",
    "I'll use the intersection symbol $\\cap$ to represent the idea of **and**. For **or** I will use\n",
    "the union symbol $\\cup$.\n",
    "\n",
    "\n",
    "Let's suppose I am sitting in my house thinking about \n",
    "what to do today. There are two outcomes that may happen today, $C$ and $S$. \n",
    "\n",
    "\n",
    "- C: I hang the clothes on the clothes-line.\n",
    "- S: I go into the garage and sweep up the sawdust.\n",
    "\n",
    "\n",
    "I decide to roll an 8-sided dice (an octahedron) where a $1$ or a $2$ means I will hang clothes.\n",
    "On a 6-sided dice (a cube) a $1$ or a $2$ means I will sweep up sawdust.\n",
    "These events are independent and we have several possible outcomes as follows:\n",
    "\n",
    "\n",
    "- C happens (never mind whether S happens) $P(C) \\; = \\; 1/4$.\n",
    "- S happens (never mind whether C happens) $P(S) \\; = \\; 1/3$.\n",
    "- Both C AND S happen $P(C \\; \\cap \\; S) \\; = \\; P(C) \\cdot P(S) \\; = \\; 1/12$.\n",
    "- Either C happens OR S happens OR both happen $P(C \\cap S) = 1/2$.\n",
    "\n",
    "\n",
    "To argue this is correct, let's suppose I do this experiment 144 days in a row. \n",
    "On 36 of those days, $C$ happens. On 48 of those days $S$ happens. However the\n",
    "$S$ days must be distributed across the $C$ and the $~C$ days in the same way.\n",
    "Otherwise $C$ would influence $S$ and the two would not be independent. \n",
    "So for the 36 $C$ days, 12 are also $S$ days. For the 108 $~C$ days, 36 are\n",
    "$S$ days.\n",
    "\n",
    "\n",
    "The 12 $P(C \\cap S)$ days are 1/12th of the full 144 trial days. For the \n",
    "final case we have all 36 $C$ days (12 of which are also $S$ days) plus\n",
    "36 additional $S$ days (with no $C$). That is a sum of 72 or half the total days.\n",
    "So this was a little bit more involved to be sure not to double count the \n",
    "days when both $C$ and $S$ happen. Notice that 1/3 + 1/4 = 7/12; so we\n",
    "do not have the simple answer for the \"or\" case of adding up the \n",
    "respective probabilities. This is sensible since if their respective \n",
    "probabilities were 8/10 and 9/10 we would get a bad-idea-addition-OR\n",
    "probability of 1.7.\n",
    "\n",
    "\n",
    "One more way to say this: In the example above, $P$ and $S$ are independent of one another; \n",
    "and this means they are *not* mutually exclusive outcomes. Both can happen; or neither can happen\n",
    "on any given day. Addition of probabilities for multiple outcomes\n",
    "applies *when those outcomes are mutually exclusive of one another*. That is, \n",
    "only one of { *outcome A*, *outcome B*, *outcome C* } can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 3: The Sum of Independent Probabilities for 'OR' Questions\n",
    "\n",
    "\n",
    "The probability of a final outcome that can be arrived at by many independent pathways is the sum of \n",
    "the probabilities of each of those pathways. This is the notion of 'counting the ways' that something \n",
    "may occur.\n",
    "\n",
    "\n",
    "Example: What is the probability of three dice adding to 16? The following are the only ways to do this \n",
    "(where the order of rolls matters; the dice are painted blue, red and yellow for example):\n",
    "\n",
    "\n",
    "* 5 + 5 + 6 with probability $(1/6)^3$ (Only one way: Blue = 5, Red = 5, Yellow = 6)\n",
    "* 5 + 6 + 5 with probability $(1/6)^3$ (Blue = 5, Red = 6, Yellow = 5)\n",
    "* 6 + 5 + 5 with probability $(1/6)^3$ ...\n",
    "* 6 + 6 + 4 with probability $(1/6)^3$ ... and so on ...\n",
    "* 6 + 4 + 6 with probability $(1/6)^3$ ... for all possible ways ...\n",
    "* 4 + 6 + 6 with probability $(1/6)^3$ ...\n",
    "\n",
    "\n",
    "Sum of probabilities = P(3 dice sum to 16) = 1/36.\n",
    "\n",
    "\n",
    "The probability of multiple-pathway events is the sum of the probabilities of the paths.\n",
    "This is the \"OR\" rule which corresponds to addition of probabilities. These ways are \n",
    "mutually exclusive of one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 4: Discrete versus Continuous Probability Distributions\n",
    "\n",
    "\n",
    "Discrete probability can be defined as: Certain results have non-zero probabilities. \n",
    "There are an integer number of dice and the probability of rolling all 6's is equal to \n",
    "((1 way of success) divided by (6 raised to the number of dice = the number of possible \n",
    "throws)). This number may be small but it is always non-zero. \n",
    "\n",
    "\n",
    "In contrast, continuous probability has the property that the probability of a precise \n",
    "result is always zero. Probabilities only become non-zero over ranges of possible outcomes. \n",
    "In this case we have to integrate a probability density function over some range to arrive \n",
    "at a probability of an outcome in that range. \n",
    "\n",
    "\n",
    "Example: What is the temperature on the surface of a stove? \n",
    "\n",
    "\n",
    "If we have a thermometer with a digital readout and one digit after the decimal point, \n",
    "then our temperature reading is discrete. The probability might be .000009 of measuring \n",
    "a temperature of 124.2 degrees. This is a discrete example, because of the limits of\n",
    "the thermometer, not the stove.\n",
    "\n",
    "\n",
    "But suppose we have a very precise thermometer that gives us a range of two temperatures.\n",
    "It tells us the stove is between 94.7822000 and 94.7823000 degrees. It's a range, get it? \n",
    "Now the idea is we are moving from the discrete case to the continuous case. The abbreviation\n",
    "is still **pdf** but it is going from a distributiong to a (continuous) density of\n",
    "probabilities. When the pdf is a continuous smooth curve we think in terms of definite\n",
    "integrals to get a probability within the limits of integration.\n",
    "\n",
    "\n",
    "\n",
    "### Idea 5: PDFs, CDFs, and Expected Outcomes\n",
    "\n",
    "\n",
    "As noted above, a PDF describes the probability of each possible outcome of a given random \n",
    "process or experiment. In throwing dice the outcomes are countable (if infinite) and hence \n",
    "the term 'discrete' is used in contrast to 'continuous' (see above).  Associated with each \n",
    "outcome is a probability value between 0 and 1. When summing up the probabilities for all \n",
    "possible outcomes the sum should be equal to one. \n",
    "\n",
    "\n",
    "When we talk about PDFs we're really talking about idealizations or perfect experiments \n",
    "or theoretical values, all equivalent to one another. In this way a PDF is a means to \n",
    "reaching idealized conclusion, knowing that an actual experiment may turn out differently \n",
    "that the calculated \"expectation\". \n",
    "\n",
    "\n",
    "To take an obvious example we can ask what is the PDF for a single die roll? Here it is \n",
    "easy to enumerate the possible outcomes: \n",
    "The die can come up with 1, 2, 3, 4, 5, or 6 six pips showing. \n",
    "Each has PDF = 1/6 so the PDF might be written { 1/6, 1/6, 1/6, 1/6, 1/6, 1/6 }. \n",
    "These sum to 1.\n",
    "\n",
    "\n",
    "Now take that same die and add some more pips to each side as needed so that the possible \n",
    "\"values\" on the up-face are 1, 4, 9, 16, 25, and 36. The PDF is now... identical: \n",
    "{ 1/6, 1/6, 1/6, 1/6, 1/6, 1/6 }. Only now the six probabilities correspond to the new \n",
    "values 1, 4, 9, etcetera. We could say that the values on the die faces do not matter.\n",
    "They really only serve to distinguish the results and could just as easily be letters \n",
    "or symbols. \n",
    "\n",
    "\n",
    "It is when we turn from pdfs (probabilities) to things that we measure (expectations)\n",
    "that the numbers on the faces of the dice become important.\n",
    "\n",
    "\n",
    "\n",
    "#### CDF or Cumulative Distribution Function\n",
    "\n",
    "\n",
    "The CDF is a sum of the PDF values. Clearly \n",
    "since any good PDF accounts for all possible outcomes, the CDF must arrive at a \n",
    "value of 1 when summed over all these outcomes. We can verify that \n",
    "a PDF is ''possibly acceptable'' by checking \n",
    "that the CDF = 1 when summed over all possible outcomes. Of course if there are an \n",
    "infinite number of possible outcomes then this becomes an infinite sum, which can \n",
    "be fun!\n",
    "\n",
    "\n",
    "Note that CDF = 1 does not imply that the PDF is correct, only that it *might* be correct.\n",
    "\n",
    "\n",
    "#### Expectation\n",
    "\n",
    "\n",
    "Once we have arrived at a correct PDF we can calculate the Expected Outcome \n",
    "or *Expectation* just as above.  Expectations are for Quantities of \n",
    "Interest. In the above example this was the number of rolls required to finish \n",
    "the game. An Expectation is equal to the sum across all possible outcomes of \n",
    "the PDF for a given outcome times the Quantity of Interest at that outcome. \n",
    "\n",
    "\n",
    "$$E = \\sum_i Quantity(i) \\cdot PDF(i)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating the PDF (via the 1-die game) in three stages\n",
    "\n",
    "\n",
    "#### Stage One: Doing a tedious experiment.\n",
    "\n",
    "\n",
    "Let's *calculate* an expectation value for an experiment using empirical means: We run the experiment \n",
    "many times to determine what is the likely outcome. The single-die game the question is: How many times \n",
    "should I expect to roll a single die before it comes up three? We play the game once and it requires 7 rolls. \n",
    "We play a second time and it requires 2 rolls. The third time: 84 rolls, a string of bad luck. And so on. \n",
    "Each time we write down the number of rolls it took and we do this experiment 20,000 times. Our results look \n",
    "like $\\;7, 2, 84, 19, 3, 1, 5, 8, 2, 1, 14, ... , 17\\;$: 20,000 values. \n",
    "\n",
    "\n",
    "To find the average: Add up all 20,000 results and divide by 20,000: How many rolls are required on average.\n",
    "\n",
    "\n",
    "\n",
    "#### Stage Two: A bit easier method\n",
    "\n",
    "\n",
    "Before we begin we create a tally sheet of empty boxes. Beside each tally box is a number: 1, 2, 3, and so on:\n",
    "The number of rolls required to finish each trial game. We do the same experiment 20,000 times but now \n",
    "each time the game finishes: we simply make a mark in the appropriate tally box. When it takes five rolls \n",
    "we put a mark in the \"5\" box. When it takes 14 rolls we put a mark in the \"14\" tally box. We need maybe \n",
    "100 boxes (but we can add more if we have a really bad game that takes 113 rolls).\n",
    "\n",
    "\n",
    "Now with 20,000 trials done the average process is simpler. Suppose that in tally box \"1\" there are 3,419 \n",
    "marks. Rather than adding up \"1\" 3,419 times (as we did in Step One) we just multiply 1 x 3,419 to get 3,419. \n",
    "If box \"2\" has 2,977 marks we add (2977 * 2) to our total. So on for 3, 4, etcetera. If tally box 7 has 219 \n",
    "marks: Add 7 * 219. The tally boxes simply make the sum easier. At the end: Suppose the sum is 121,432. \n",
    "Just as before divide by 20,000 to get the expected number of rolls required to get a 6.\n",
    "\n",
    "\n",
    "Take a look at the 100 tally boxes. The first box will have the most ticks, the second box fewer, the third \n",
    "box fewer still, and so on. It will look like a long tail getting smaller as the tally box numbers get larger. \n",
    "This is a probability distribution: It is more likely to take three rolls than it is to take 41 rolls. \n",
    "There is one thing that distinguishes this plot from a PDF, however: If the values in the tally boxes are \n",
    "numbers like 3,419 and 2,753 and so on, they do not add up to one. That is, they are relative probabilities \n",
    "but not absolute probabilities in the \"zero to one\" range. But we can fix that next. \n",
    "\n",
    "\n",
    "#### Stage Three: The PDF\n",
    "\n",
    "\n",
    "The third time we do the experiment we proceed as before with the tally boxes. Before after 20,000 trials \n",
    "our procedure was to sum up Sum = {tally box marks x box number}. Then at the end expectation = Sum / 20,000.\n",
    "This time we use a different order: First we divide the tally box count by 20,000 and to get a fraction \n",
    "less than one; and then we multiply by the box number. It is the same calculation as above, just done in \n",
    "a different order. \n",
    "\n",
    "\n",
    "This experiment builds an observational PDF. Why? First, the modified tally scores are now fractions of 20,000: \n",
    "3,419/20,000, 2,753/20,000, etcetera. Add them all up and we are certain to get 1 since there are a total of \n",
    "20,000 marks. Our expectation E for the roll-one-six game can be written in terms of this PDF as:\n",
    "\n",
    "\n",
    "$$\\;E = \\sum_i i \\cdot \\frac{tally \\; score(box \\; i)}{20,000} = \\sum_i i \\cdot PDF(i)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sixes Dice Game\n",
    "\n",
    "\n",
    "Ever play the board game **Risk**? How about **Yahtsee**? In many games like these we do well when we roll lots of sixes. One day I found myself with a large pile of dice and I invented a silly game that requires no skill. I rolled all of the dice at once and set aside any that came up 6. I then rolled the remaining dice and repeated the set-aside procedure. I counted how many rolls it took to get all sixes; and that was my score. Then I repeated the game to see if I could get a better (lower) score. \n",
    "\n",
    "Let's call this game Sixes. After awhile I wondered how many throws of the dice it ''ought'' to take to finish one round of the game. That is the problem addressed on his page. Let's call that number $T$ and since it depends on how many dice one begins with (say $N$) let's make it explicit that $T$ depends upon $N$.\n",
    "\n",
    "***In the game of sixes: What is $T_N$?***\n",
    "\n",
    "\n",
    "### Expectation\n",
    "\n",
    "We call this number $T$ an *expectation* in probability. In going through this solution we will try and motivate how expectations can be calculated. \n",
    "\n",
    "Notice that as we play a round of Sixes: We don't need to set aside dice that have come up six; we can keep rolling them provided we remember who has come up six at least once. So another way to state the problem is: What is the expected number of throws T for N dice such that each die comes up $6$ at least once. \n",
    "\n",
    "Kilroy picture of many dice rolled, some sixes\n",
    "\n",
    "### What Sixes is *not*\n",
    "\n",
    "If we begin with $N$ dice and roll all of them in a group and count the sixes, and then roll all of them again in a group, and add in how many sixes this time, and so on: We will not be playing the game described above. The reason for this I leave to you to figure out.\n",
    "\n",
    "\n",
    "### A fun problem\n",
    "\n",
    "First notice it will be pretty easy to write a computer program that plays many games of Sixes and averages the results. Hence we have a way of testing any solution for $T$ that we come up with; and we can determine if a given solution is accurate or not. \n",
    "\n",
    "I found a first solution that worked pretty well for large numbers of dice $N$, for around 40 dice. I imagined at the time that I had solved the problem... but then I found that my solution failed miserably for smaller values of $N$, particularly for one or two or three dice. \n",
    "\n",
    "I thought about it some more and decided that my reasoning was very suspicious! So I set about refining my solution to the one presented here. The one presented here agrees very well with the computer test; so that is encouraging. It was very fun to work on.\n",
    "\n",
    "### Method\n",
    "\n",
    "Let's face it, the novelty of playing Sixes with 30 dice wears thin in a hurry, and that's just one data point.  I have used a computer program that rolls many dice very quickly to arrive at my empirical / experimental values for $T_N$. I can then compare these results to theoretical tries to see how my hypotheses look. The theoretical values are calculated using a different computer algorithm (working from a formula) where I wish to verify $T_{N, theory}$ with $T_{N, experiment}$ for $N = 1, 2, 3, ..., 150, ...$.\n",
    "\n",
    "#### First solution (wrong)\n",
    "\n",
    "There are proper rules for probability, and then there are made-up hokey rules. I'll start with made-up hokey rules to arrive at that first incorrect solution. Here is my supposition leading down the garden path to wrongness:\n",
    "\n",
    "\n",
    "***What is $T_N$ if the dice behave perfectly and are not discrete?***\n",
    "\n",
    "\n",
    "This is a bizarre and unclear question. Perfect behavior implies the dice are discrete; a die can come up a $6$ or not; it can not come up $30%$ $6$ and $20%$ $5$ and $50%$ $1$, $2$, $3$, or $4$, nor can it come up $5.217$.  \n",
    "\n",
    "\n",
    "What I ''tried'' to mean by the phrase 'not discrete': Suppose the dice are capable of being cut into pieces, 'fractional dice'. Suppose further that of any number of dice (like say $19.4$ of them) can be thrown and exactly $1/6$ of these will come up $6$ and will be set aside. What I ''tried'' to mean by 'perfectly behaved' is that they distribute their rolls evenly among all $6$ possible outcomes, even though there is a non-integral number of dice. Notice that any non-integral number of dice can be made roughly integral by multiplying by some fixed constant, like 100 billion, so this line of thinking might be ok in some large-N limit sense.\n",
    "\n",
    "\n",
    "Needless to say the fraction of dice that survive one roll are not \"more disposed\" to come up six on the next.\n",
    "\n",
    "\n",
    "\n",
    "#### First solution aside\n",
    "\n",
    "I am essentially using a large-numbers approximation in this first solution, assuming a perfect 1/6th of the dice can be eliminated. Small wonder this solution does not work well for small numbers of dice! \n",
    "\n",
    "kilroy need Idea 4 here\n",
    "\n",
    "\n",
    "#### First solution: assumption\n",
    "\n",
    "Suppose I start with $N$ dice and $1/6$ of them unfailingly turn up $6$. After one throw I have a remainder $R_1 = (5/6) \\cdot N$ dice remaining (since all the others came up 6). After two throws I have a remainder of $R_2 = (5/6) \\cdot (5/6) \\cdot N$ dice remaining. After $t$ throws I have $R_t = N \\cdot (5/6)^t$ dice remaining. If I set $R_t = 0$ (no dice left) and solve for $t$, that will be $T_N$. Simple!\n",
    "\n",
    "\n",
    "$$(5/6)^t \\cdot N = 0$$\n",
    "\n",
    "\n",
    "This unfortunately has the solution $t = + \\infty$. Never fear: Let us commence to cheating. I assume that once I have one die left it will take me 6 more throws for it to come up a 6 so I can eliminate ''it'' and be done. So I can use my above idea to determine how long it takes me to have one die left, then add six more throws and I'll have $T_N$.\n",
    "\n",
    "\n",
    "$$(5/6)^t \\cdot N = 1$$ gives $$t = \\ln(N) \\;/ \\;\\ln(6/5) = 0.182 \\;\\ln(N)$$\n",
    "\n",
    "$$T(N) = 0.182 \\cdot \\ln(N) + 6$$\n",
    "\n",
    "\n",
    "Simple! But this is not quite correct. I had that last die 'riding along' (getting thrown each time) on my way to eliminating the other $N - 1$ dice... and it might very well have come up $6$ at least once in which case... I'm done before I'm done... unless some other die managed to ride along without coming up $6$... but this is becoming muddled...\n",
    "\n",
    "\n",
    "#### More bad ideas...\n",
    "\n",
    "So far, not so good. What if I use the 'fractional dice' idea and throw N dice (eliminating 6s) until just $1/2$ a die remains? Maybe then it goes poof and becomes zero dice:\n",
    "\n",
    "\n",
    "$$(5/6)^t \\cdot N = 1/2$$\n",
    "\n",
    "\n",
    "$$t = \\ln \\; (2N) / \\ln \\; (6/5) = 0.182 \\ln \\; (2N)$$\n",
    "\n",
    "\n",
    "This (phony) result works quite well for $N = 20, 21, 22, ...$ and larger values of $N$. Weird or what?\n",
    "\n",
    " \n",
    "To reiterate: Start with a large number of dice, assume every throw eliminates precisely $1/6$ of them (non-integer remaining dice ok...), and upon reaching $0.5$ dice remaining this collapses to zero dice. That idea gives us $T_N$.\n",
    "\n",
    "\n",
    "As I mentioned this formula does not work for smaller numbers of dice. For $N = 1$ it gives $T_1 = 3.8$ for example. What is $T_1$ really? Maybe it ought to be six, as in the previous try where I eliminated $N-1$ dice and then added six more rolls. So, ok, enough with these tones, let's next head off more carefully towards a proper solution.\n",
    "\n",
    "\n",
    "#### What is $T_1$?\n",
    "<br>\n",
    "In looking for $T_N$ let's begin with the easier task of finding $T_1$, the expected number of throws of one die needed to roll a six. To make progress let's bring in the machinery of the probability distribution function, often abbreviated ''pdf''. This function describes the probabilistic nature of \"what we're interested in\"; we consider all the possible outcomes of an experiment (a game of Sixes) and determine the probability of each. \n",
    "\n",
    "\n",
    "As we're playing Sixes with one die for now I'll determine $pdf_1(t)$, a discrete probability distribution function for \"success exactly on throw $t$\". That is, after failing to roll a 6 $(t-1)$ times, I roll a 6 on throw $t$. (The subscript 1 on $T_1$ means we play with only one die.)\n",
    "\n",
    "\n",
    "Second for the sake of form let's verify that \n",
    "\n",
    "\n",
    "$$\\sum^{\\infty}_{t=1}pdf_1(t) = 1$$\n",
    "\n",
    "\n",
    "(The sum evaluated is called a cumulative distribution function or $cdf(t)$.)\n",
    "\n",
    "\n",
    "Third, once we have a good $pdf_1(t)$ we can derive $T_1$. \n",
    "\n",
    "Kilroy need Idea 5 here\n",
    "\n",
    "\n",
    "\n",
    "#### $pdf_1(t)$\n",
    "\n",
    "After $(t-1)$ throws I have to take into account that I might already have rolled a 6. In this case the probability of ''ending'' on throw $t$ is zero because the die has been eliminated from play already.\n",
    "\n",
    "\n",
    "Using the \"counting ways\" method I imagine a die thrown $(t-1)$ times without ever coming up 6:\n",
    "\n",
    "\n",
    " __ __ __ __ __ __ __ ... __\n",
    " 1  2  3  4  5  6  7      t-1\n",
    "\n",
    "\n",
    "What rolls can go in these spaces? On throw 1 I could have got a 1 or a 2 or a 3 or a 4 or a 5, five ways. Same thing for roll 2 and for roll 3 and so on. Since these throws are independent of one another, citing [[Dice supporting probability ideas#Ideas_2|Idea 2]] and [[Dice supporting probability ideas#Ideas_3|Idea 3]] about independent events, the total number of ways of producing these desired t-1 throws is obtained by counting and then multiplying the ways for each throw: $5/6 \\cdot 5/6 \\cdot 5/6 \\cdot 5/6 \\cdot 5/6 \\cdot ... \\cdot 5/6 = (5/6)^{t-1}$ ways to not roll a six $(t-1)$ times.\n",
    "\n",
    "\n",
    "Finally having not rolled a six t-1 times in a row, what is the probability of finally rolling that six on throw $t$? Answer: $1/6$ of course. So...\n",
    "\n",
    "\n",
    "$$pdf_1(t) = (5/6)^{t-1} \\cdot (1/6)$$\n",
    "\n",
    "\n",
    "The following plot (Excel) shows this pdf function on a vertical logarithmic axis. kilroy or not.\n",
    "\n",
    "This means that as the number of throws $t$ increases, the probability of winning on throw $t$ becomes exponentially smaller. Furthermore the right-hand axis runs from 0 to 1 and shows the cumulative or summed value of the pdf (cdf = cumulative distribution function) on a linear scale. As desired, the cdf approaches 1 as $t$ increases, meaning that the probability of winning the game with one die ''eventually'' is a certainty. Finally the bumpy plot is what we could call the expectation function, $t \\cdot pdf_1(t)$. Summing this up (as described below and under \n",
    "\n",
    "kilroy need idea 5 here also \n",
    "\n",
    "produces the expected number of throws needed to win $T_1$, which will prove to be six.\n",
    "\n",
    "\n",
    "#### Is $pdf_1(t)$ normalized?\n",
    "\n",
    "Does $cdf_1(t \\rightarrow \\infty) = 1$? It better! This will mean that the probability distribution covers all conceivable outcomes since the total probability is one, a sure thing. \n",
    "\n",
    "\n",
    "$cdf_1(t \\rightarrow \\infty) = \\lim_{\\tau \\rightarrow \\infty} \\sum_{t = 1}^{\\tau} \\; \\left( \\frac{1}{6} \\right) \\cdot {\\left( \\frac{5}{6} \\right)}^{t-1}$\n",
    "\n",
    "\n",
    "\n",
    "This can be evaluated using a very pretty idea that I found in Apostol's Calculus-I: The series \n",
    "\n",
    "\n",
    "$S = 1 + a + a^2 + a^3 + a^4 + ... + a^t$\n",
    "\n",
    "\n",
    "can be multiplied by $-a$ to produce a new series \n",
    "\n",
    "\n",
    "$$T = -a - a^2 - a^3 - a^4 - a^5 - ... - a^t - a^{t+1}$$\n",
    "\n",
    "\n",
    "and when $S$ and $T$ are added together only the first and last terms remain:\n",
    "\n",
    "\n",
    "$$S + T = 1 - a^{t+1}$$\n",
    "\n",
    "\n",
    "The rest of the terms 'telescope' away to nothing. Hence (writing $T$ as $-aS$): \n",
    "\n",
    "\n",
    "$$S \\cdot (1 - a) = 1 - a^{t+1}$$\n",
    "\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "$$S = \\frac{1 - a^{t+1}}{1 - a}$$\n",
    "\n",
    "\n",
    "As a result the cumulative distribution function sum above becomes\n",
    "\n",
    "\n",
    "$$cdf_1(t \\rightarrow \\infty) = \\left( \\frac{1}{6} \\right) \\lim_{\\tau \\rightarrow \\infty} \\frac{1 - (5/6)^{\\tau+1}}{1 - (5/6)} = \\left( \\frac{1}{6} \\right) \\cdot 6 = 1$$\n",
    "\n",
    "\n",
    "This cdf corresponding to $pdf_1(t)$ converges nicely to one as $t$ goes to infinity: If I throw a single die for a really really long time, sooner or later I'll roll a six. So far so good.\n",
    "\n",
    "\n",
    "#### What is $T_1$?\n",
    "\n",
    "We can next calculate the expectation value for the single die version of the sixes game: \n",
    "\n",
    "\n",
    "$$T_1 = \\sum_{t=1}^\\infty t \\cdot pdf_1(t).$$\n",
    "\n",
    "\n",
    "For supporting remarks refer to kilroy idea 5\n",
    "\n",
    "\n",
    "#### How to calculate $T_1$\n",
    "\n",
    "\n",
    "To get to $T_1$ we'll take the limit $n \\rightarrow \\infty$ for a finite sum of $n$ terms. To make notation easier I'll use $a$ and $b$ for the two fractions $1/6$ and $5/6$. The strategy here is similar to the telescoping series idea used to sum the pdf.\n",
    "\n",
    "\n",
    "$$T = \\sum_{i=1}^{n} i \\cdot \\frac{1}{6} \\cdot \\frac{5}{6}^{i-1} = a \\sum_{i=1}^{n}i\\cdot b^{i-1}$$\n",
    "\n",
    "\n",
    "$$b \\cdot T = a \\sum_{i=1}^{n}i \\cdot b^{i}$$\n",
    "\n",
    "\n",
    "$$T(1-b) = a \\sum i \\cdot \\left( b^{i-1}-b^{i} \\right) = a\\cdot \\left( 1 - b + 2b - 2b^2 + 3b^2 - 3b^3 + ... + (n-1)b^{n-2} - (n-1)b^{n-1} + nb^{n-1} - nb^n \\right)$$\n",
    "\n",
    "\n",
    "$$T(1-b) = a\\left( 1 + b + b^2 + ... +b^{n-1}\\right) - a n b^n = \\left( \\sum_{i=1}^{n}a \\cdot b^{i-1} \\right) - a \\cdot n \\cdot b^{n}$$\n",
    "\n",
    "\n",
    "In this final (rightmost) expression, the sum comprising the left term of the two terms is the finite version of the cdf sum done up above which in the limit was equal to 1. What about the right expression, a product of $n$ and $b$ raised to the $n$?  Noting that $b$ is less than one, I need to claim that $b^n$ goes to zero \"hard\" as $n$ increases so the product $n \\cdot b^{n}$ goes to zero as $n$ tends towards infinity \n",
    "\n",
    "kilroy: need Exponential limit|a proof from Professor Apostol). \n",
    "\n",
    "That done I continue by dividing both sides of the previous equation by (1-b) and taking the limit of $n$ going to infinity:\n",
    "\n",
    "$$T_1 = \\lim_{n \\rightarrow \\infty} T = \\frac{1}{1-b} - \\lim_{n \\rightarrow \\infty} \\frac {a \\cdot n \\cdot b^{n}}{1-b} = 6 - \\lim_{n \\rightarrow \\infty} n \\cdot b^{n} = 6.$$\n",
    "\n",
    "If I throw a die a bunch of times I should expect to have to throw it $6$ times to get a $6$. This is good news because I should also expect to throw it six times to get a $5$, a $4$, a $3$, a $2$, and a $1$. After six throws I've covered all equivalent possibilities with no favorites and so this makes sense.\n",
    "\n",
    "Up above under '''\"More bad ideas...\"''' I got $T_1 = 3.8$ which makes considerably less sense; so I'm encouraged to think that I'm now on the right track. It remains to do the final jump to a pdf for $N$ dice and I'll have a solution for $T_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is $T_N$?\n",
    "\n",
    "Going from $T_1$ to $T_N$ is not trivial but it's not too horrible either. Just as for $T_1$ we work out an appropriate PDF for finishing the game on throw $t$ as a function of $N$.  Then we can verify that this PDF sums to 1 and then try and calculate $T_N$.  In this more general case the result may not be quite so pretty... we shall see. \n",
    "\n",
    "\n",
    "In the double sum that follows note that the index $t$ runs from 1 to $\\infty$ being the number of throws needed to finish the game, and the index $i$ runs from $1$ to $N$, the number of dice we play the game with. Since the pdf is a function of throw number (what is the probability of finishing the game on throw $t$) it is written as $pdf_N(t)$ for a game of Sixes that begins with $N$ dice. \n",
    "\n",
    "\n",
    "I first describe the form of the PDF: Suppose that the game is to end on throw $t$. Then there are between one and $N$ dice remaining after $t-1$ throws, i.e. $N$ cases to consider. For each case there is a certain probability of the game ending on throw $t$.  \n",
    "\n",
    "kilroy From [[Dice supporting probability ideas#Idea 3|Idea 3]] \n",
    "\n",
    "the PDF for throw $t$ must therefore be a sum from $1$ to $N$ of the odds of rolling one $6$, two $6$'s, three $6$'s, ..., $N$ $6$'s, all weighted by the probability of having respectively those many dice remaining after $t-1$ throws. This is a sum over the number of ways to finish the game on throw $t$. In other words the PDF is the sum of $N$ individual probabilities of finishing on throw $t$ that account for all the ways of ''not'' having finished the game after the previous $t-1$ throws.  \n",
    "\n",
    "\n",
    "Rather than bulldoze ahead to a form for the PDF I would like to proceed a little bit more retrograde as it makes the writing easier to do. So I will claim that the PDF can conveniently be written as a sum (over $N$ dice) of the product of four functions, to be elaborated in a bit: $pdf_N(t) = \\sum_{i=1}^N F_1(i) \\cdot F_2(i) \\cdot F_3(i, t) \\cdot F_4(i, t)$\n",
    "\n",
    "\n",
    "so that (assuming it to be a valid PDF) the solution for $T_N$ will introduce a second sum over an infinite number of throws, to look like this: \n",
    "\n",
    "\n",
    "$$T_N = \\sum_{t = 1}^{\\infty} \\sum_{i = 1}^N t \\cdot F_1 \\cdot F_2 \\cdot F_3 \\cdot F_4$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$$F_1 = {\\left( \\frac{1}{6} \\right)}^i$$         \n",
    "\n",
    "\n",
    "$$F_2 = \\binom{N}{i}$$\n",
    "\n",
    "\n",
    "$$F_3 = { \\left( \\frac{5}{6}^{t-1} \\right) }^i$$\n",
    "\n",
    "\n",
    "$$F_4 = { \\left( 1-\\frac{5}{6}^{t-1} \\right) }^{N-i}$$\n",
    "\n",
    "\n",
    "So here we get to go through the justification. $F_1$ is the probability of rolling $i$ sixes on throw $t$. $F_2$ is the number of ways of choosing those $i$ dice from among $N$. Hence the remaining two functions multiplied together had better give the probability of winding up with $i$ dice after $t-1$ throws.\n",
    "\n",
    "\n",
    "In fact if we assume that exactly $i$ dice have survived to throw $t$ (and we've already accounted for the number of ways of selecting them) the last two factors $F_3$ and $F_4$ are respectively the probabilities that the $i$ dice never came up $6$ and the remaining $N - i$ dice each came up $6$ at least once in $t-1$ throws. So in words the probability of finishing the game precisely on throw $t$ is the sum of $N$ ways of winning on throw $t$, where the index $i$ in this sum is the number of dice that have survived to throw $t$ without ever coming up six, and where to win on throw $t$ they must all come up six on that throw. In order to reach these $i$ dice remaining, first they must never have come up six in $t-1$ throws, and the remaining $N - i$ dice must all have come up six at least once in $t - 1$ throws. \n",
    "\n",
    "\n",
    "More justification for these last two terms in the PDF is given as kilroy Dice supporting probability ideas#Idea 6|Idea 6. The solution to the problem therefore is explicitly:\n",
    "\n",
    "\n",
    "$$T_N = \\sum_{t = 1}^{\\infty} \\sum_{i = 1}^N t \\cdot {\\left( \\frac{1}{6} \\right)}^i \\cdot \\binom{N}{i}\n",
    "\\cdot { \\left( \\frac{5}{6}^{t-1} \\right) }^i \\cdot { \\left( 1-\\frac{5}{6}^{t-1} \\right) }^{N-i}.$$\n",
    "\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "$$\n",
    "T_N = \\sum_{t = 1}^{\\infty} \\sum_{i = 1}^N \\frac{t}{6^{it}} \\cdot \\binom{N}{i} \\cdot 5^{it-i} \\cdot { \\left( \\frac{6^{t-1}-5^{t-1}}{6^{t-1}} \\right) }^{N-i}\n",
    "$$\n",
    "\n",
    "\n",
    "kilroy sixes and fives picture\n",
    "\n",
    "This answer may have some nicer form but I haven't found it yet. (Some more things to do here are listed below, including that idea.) The computer of course can give a precise \"actual number\" for $T_N$ by adding up the double sum (with only a finite number of terms for $t$) and so for example if you begin with $19$ dice it should take about ____missing____ throws to win the game on average. \n",
    "\n",
    "Incidentally if you'd like to look at another problem along these lines, it is cast (sic) Recruiter Problem|here as the \"recruiter problem\".\n",
    "\n",
    "\n",
    "### Loose Ends\n",
    "\n",
    "\n",
    "* '''Can TN be simplified algebraically? Approximated? I don't know just yet.'''\n",
    "* '''Does <T> match <Tbad> for large N? T-cheater using N+1?'''\n",
    "* '''Digressions: BinThm, Fibonacci, Generating functions for <t^n>'''\n",
    "* '''More messy combinatorics: Derive expected sum of N dice'''\n",
    "* '''More on Radioactive Decay as a random process with an eye to Zircon'''\n",
    "* I notice that our dice behave like unstable atoms waiting around to fall apart, where throws are ticks of a clock. Is my initial \"wrong\" guess more applicable when we consider huge numbers of atoms, i.e. is it a suitable approximation for large N?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Probability Ideas\n",
    "\n",
    "This is not comprehensive! But it runs through some probability theory away from the problem that is \n",
    "the subject of this Notebook. \n",
    "\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "- '''Throw''': The act of selecting a die face at random. Equivalent to 'experiment' more generally.\n",
    "- '''Roll''': The result of a throw (a number from 1 to 6). \n",
    "- '''pdf''': Probability distribution function, a description of \"outcomes\" in terms of their relative probabilities.\n",
    "\n",
    "\n",
    "Hence \"I threw a die, I rolled a six, and the pdf says there was a one in six probability of this outcome.\" This terminology differentiates observation (throw) from result (roll). Pdfs are discussed in more depth below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermezzo on the birthday experiment\n",
    "\n",
    "\n",
    "$n$ people arrive at a party and everyone shakes hands with everyone else. As they do so each person \n",
    "announces their birthday. What is the probability that at least one pair of persons have the same\n",
    "birthday? We take a year to be $365$ days long and we take birthdays to be uniformly, randomly\n",
    "distributed. \n",
    "\n",
    "\n",
    "The probability of one pair of people having the same birthday is $1/365$. The sum of probabilities\n",
    "will be $1/365 + 1/365 + \\dots + 1/365 = n/365$. However this number is too high because we do not\n",
    "take into account cases where more than one pair of people have the same birthday. Maybe we should try\n",
    "with a large number of trials $a$ (each time bringing $n$ people together). \n",
    "\n",
    "\n",
    "Then we can say: \n",
    "\n",
    "\n",
    "- 1 and 2 share a birthday on $a/365$ trials. $a-a/365$ trials remain. \n",
    "  - We don't care if others share birthdays on those first $a/365$ trials\n",
    "- $\\frac{a-a/365}{365}$ trials now have 1 and 3 sharing a birthday...\n",
    "  - So we can add this to our cumulative probability\n",
    "  - And now there are $a - a/365 - (a - a/365)/365$ trials remaining\n",
    "- And so on until we exhaust all $n-choose-2$ possible pairs\n",
    "  - Oh dear... that seems a bit messy\n",
    "  \n",
    "\n",
    "From this follows some good news and some bad news. And some more good news. \n",
    "\n",
    "\n",
    "The bad news is that to go from the idea above to a good answer takes some arithmetic work. \n",
    "\n",
    "\n",
    "(kilroy add this in)\n",
    "\n",
    "\n",
    "The good news is that the answer from the arithmetic is perfectly possible and makes plenty of sense\n",
    "once we get there. \n",
    "\n",
    "\n",
    "The other good news is actually that I discovered a probability principle: When we have these situations\n",
    "with lots of ways of succeeding: Look at the complementary probability. That is: Given a *success*\n",
    "involving many paths, *this* **OR** *this* **OR** *this* ... **OR** *this*: Look at the converse\n",
    "situation *~this* **AND** *~this* **AND** *~this* ... **AND** *~this*. When we have **AND** we \n",
    "simply multiply probabilities. That gives the probability of failure; and probability of success\n",
    "is simply one minus that.\n",
    "\n",
    "\n",
    "In our case \n",
    "\n",
    "\n",
    "$P(n\\;people\\;have\\;at\\;least\\;one\\;shared\\;birthday)\\;=\\;P(two\\;people\\;do\\;not\\;share\\;a\\;birthday)^{pairs\\;of\\;people}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the matter of \"Either C or S or BOTH\" occurs we have (36 + 36)/144; so this means at least one of S and C \n",
    "happens 1/2 of the time. Notice that P(C) + P(S) = 1/4 + 1/3 = 7/12 which ''overcounts'' because there are \n",
    "some days on which both occur. P(C) + P(S) counts those days twice, so we subtract away one of those as \n",
    "P(C AND S) to arrive at the correct result of 1/2. 7/12 - 1/12 = 1/2, so 1/12 must be P(C AND S), \n",
    "which is P(C) x P(S); so the \"AND\" implies multiplication of independent probabilities.\n",
    "\n",
    "\n",
    "This *counting of ways* means that we count the number of possible outcomes, we count the number of desired outcomes, \n",
    "and set the probability of the desired outcome as the ratio of desired to possible outcomes. In the process we \n",
    "have to take care not to count some outcomes more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the following two events in the *dependent* case:\n",
    "\n",
    "\n",
    "* D: I drive from my home to the post office parking lot.\n",
    "* W: I walk from the parking lot into the post office.\n",
    "\n",
    "\n",
    "In this case W is dependent on D because I only have the option to walk inside if I first drive to the parking lot. Suppose P(D) is 1/4 and P(W) is 1/3, but we require that W can only happen if D happens. In fact a better way to write this is P(D) = 1/4, P(W | D) = 1/3, and P(W | !D) = 0. This last means there is no way W can happen if \"Not-D\" happens. If I don't drive to the parking lot I can't possibly then walk inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the same questions to compare with the independent case above: \n",
    "\n",
    "\n",
    "* Outcome D (never mind whether W happens) has probability 1/4 just as before.\n",
    "* Outcome W (only possible if D happens) has probability P(D)*P(W|D) = 1/12. \n",
    "* Probability both D and W happen = P(D)*P(W). This will be 1/12 as in the previous line.\n",
    "* Either D happens or W happens or both happen = P(D) = 1/4.\n",
    "\n",
    "\n",
    "Again the last line requires some thought. W will happen only if D happens. But if D happens then the result will be true regardless of whether W happens, so W is irrelevant to the total question. In this second example I've looked at how a dependency can completely alter the manner in which probabilities are calculated in relation to the first example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the third case, P(D and W), comes out 1/12 just as above where C and S were independent of one another. This is interesting because there is (in both cases) a dependence built into the use of the word 'and'. That is, we have a subtle distinction between the nature of the events and whether they depend upon one another. In the first case with C and S independent, the outcome question \"Do C and S both happen?\" ties them together. In the second case \"Do D and W both happen\" also ties together two probabilities, but W is already tied to D by it's definition. The nature of the question we ask about the final outcome has built into it the nature of the constituent events.\n",
    "\n",
    "\n",
    "The main point here is that independent events (including the numbers rolled on dice) are not too bad to work with in questions of probability. The particular result of note is that the probabilities of independent events are multiplied to arrive at the probability that they both occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''The probability of several independent events all happening is the product of the probabilities of the individual events. This is the \"AND\" rule which corresponds to multiplication of probabilities.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final example here: What number do I expect to roll on a single throw of a die?\n",
    "\n",
    "As noted the PDF for a single die roll is {1/6, 1/6, 1/6, 1/6, 1,6, 1/6}. My expected roll is therefore \n",
    "\n",
    "\n",
    "$1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + ... + 6 \\cdot \\frac{1}{6} = 3.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average a single die comes up 3.5 (even though it never comes up 3.5 in practice). Similarly if I evaluate my die throw as the square of the number of pips, that is, if my Quantity of Interest is $\\;pips^2$ then my expected outcome is \n",
    "\n",
    "\n",
    "$1 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + ... + 36 \\cdot \\frac{1}{6} = 15.17$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea 6 for the Sixes game\n",
    "\n",
    "$\\; F_3 = { \\left( \\frac{5}{6}^{t-1} \\right) }^i$ is a combination of two ideas. \n",
    "\n",
    "\n",
    "First it says that the probability for rolling not-six on a single die ''t - 1'' times in a row is $\\;\\left( \\frac{5}{6} \\right)^{t-1}$. Call that number ''Q''. Second, it says that the probability of doing this with a collection of ''i'' dice is $\\;Q^i$. This is a double-application of '''Idea 2''' above. \n",
    "\n",
    "\n",
    "$\\; F_4 = { \\left( 1-\\frac{5}{6}^{t-1} \\right) }^{N-i}$ is a similar maneuver that also uses '''Idea 1''' about the complementary nature of P(X) and P(!X) where they add up to one. First the probability for not rolling not-six on a single die ''t - 1'' times in a row is one minus the probability of rolling not-six on that single die ''t - 1'' times in a row, which we have in $\\;F_3$. Again we raise this to the power of the remaining number of dice involved (''N - i'') to get the probability that all of these dice have come up six at least once in ''t'' throws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary then, we've selected out ''i'' dice and determined the probability that they could all come up six on throw 't' in terms $\\;F_1$ and $\\;F_2$, and we use terms $\\;F_3$ and $\\;F_4$ to moderate the result by the probability that we could have arrived (after ''t - 1'' throws) with ''i'' dice remaining in our game of Sixes. We use $\\;F_3$ to represent the probability that ''i'' dice never came up six in ''t - 1'' throws and we multiply this probability by the independent probability $\\;F_4$ that the remaining ''N - i'' dice all came up six at least once (and were eliminated from the game) in those ''t - 1'' throws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 dice and 1000000 rounds gives Tn = 19.371724 with max/min =  91 4\n"
     ]
    }
   ],
   "source": [
    "## Empirical calculation of Tn \n",
    "## Original command line give start, stop, increment for n\n",
    "\n",
    "n = 17\n",
    "rounds = 1000000\n",
    "\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "sum = 0\n",
    "max = 0\n",
    "min = 0\n",
    "for trials in range(rounds):\n",
    "    nLeft = n\n",
    "    rolls = 0\n",
    "    while nLeft > 0:\n",
    "        rolls += 1\n",
    "        for d in range(nLeft):\n",
    "            if r.randint(1,6) == 6:\n",
    "                nLeft -= 1\n",
    "    sum += rolls\n",
    "    if trials == 0:\n",
    "        max = rolls\n",
    "        min = rolls\n",
    "    else:\n",
    "        if rolls > max: max = rolls\n",
    "        if rolls < min: min = rolls\n",
    "\n",
    "# one could introduce a hypothesis function for comparison, for example:\n",
    "# hypothesis = log(1./(float)(2*ND)) / log(5./6.);\n",
    "experiment = (float)(sum) / (float)(rounds)\n",
    "print n, 'dice and', rounds, 'rounds gives Tn =', experiment, 'with max/min = ', max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1     pdf = 1.0     T = 6.0\n",
      "N = 2     pdf = 1.0     T = 8.72727272727\n",
      "N = 3     pdf = 1.0     T = 10.5554445554\n",
      "N = 4     pdf = 1.0     T = 11.9266962546\n",
      "N = 5     pdf = 1.0     T = 13.0236615076\n",
      "N = 6     pdf = 1.0     T = 13.9377966973\n",
      "N = 7     pdf = 1.0     T = 14.7213415963\n",
      "N = 8     pdf = 1.0     T = 15.4069434779\n",
      "N = 9     pdf = 1.0     T = 16.0163673665\n",
      "N = 10     pdf = 1.0     T = 16.5648488613\n",
      "N = 11     pdf = 1.0     T = 17.0634684016\n",
      "N = 12     pdf = 1.0     T = 17.5205363138\n",
      "N = 13     pdf = 1.0     T = 17.942445156\n",
      "N = 14     pdf = 1.0     T = 18.3342176523\n",
      "N = 15     pdf = 1.0     T = 18.6998719821\n",
      "N = 16     pdf = 1.0     T = 19.0426729163\n",
      "N = 17     pdf = 1.0     T = 19.3653090897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import power\n",
    "from scipy.special import comb\n",
    "\n",
    "upper_N = 17\n",
    "\n",
    "for N in range(1, upper_N + 1):\n",
    "    # N is the number of dice in this game\n",
    "    infinity_t = 200\n",
    "    pdf = 0.\n",
    "    T = 0.\n",
    "    for t in range(1, infinity_t + 1):\n",
    "        Fsum = 0.\n",
    "        for i in range(1, N + 1):\n",
    "            F1 = power(1./6., float(i))\n",
    "            F2 = float(comb(N, i))\n",
    "            F3 = power(power(5./6., float(t-1)),float(i))\n",
    "            F4 = power(1.-power(5./6.,float(t-1)),float(N-i))\n",
    "            Fsum += F1*F2*F3*F4\n",
    "        pdf += Fsum\n",
    "        T += float(t) * Fsum\n",
    "    print 'N =', N, '    pdf =', pdf, '    T =', T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$T_N = \\sum_{t = 1}^{\\infty} \\sum_{i = 1}^N t \\cdot F_1 \\cdot F_2 \\cdot F_3 \\cdot F_4$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$$F_1 = {\\left( \\frac{1}{6} \\right)}^i$$         \n",
    "\n",
    "\n",
    "$$F_2 = \\binom{N}{i}$$\n",
    "\n",
    "\n",
    "$$F_3 = { \\left( \\frac{5}{6}^{t-1} \\right) }^i$$\n",
    "\n",
    "\n",
    "$$F_4 = { \\left( 1-\\frac{5}{6}^{t-1} \\right) }^{N-i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recruiter Problem: Introduction\n",
    "\n",
    "This page describes and solves a probability problem similar to the Sixes problem.\n",
    "\n",
    "\n",
    "### Problem\n",
    "\n",
    "A recruiter has a list of potential recruits ranked in order: 1, 2, 3, .... She wishes to hire $N$ of them and to make the problem simple we assume they each have the same probability $a$ of saying Yes when offered the job. She begins at the top of her list and offers the job in order to people until she has $N$ yesses. The problem is stated one way like this: How many people do we expect will get offered the job? \n",
    "\n",
    "\n",
    "A second problem considered here is how to derive $a$ given a different piece of information, namely that the probability of person $M$ in the list being offered the job is very small. This is in the spirit of the original question: \"What are my chances of getting offered the job if they are looking for 10 people at a time and they never have to ask more than 30?\"\n",
    "\n",
    "\n",
    "### What is the expected number of asks?\n",
    "\n",
    "As with the Sixes problem we take the standard 'probability distribution function' approach:\n",
    "\n",
    "\n",
    "* We're given $a$ and $N$.\n",
    "* Figure out what the probabiltiy is of hiring $N$ people on precisely $n$ asks, $P_{n}$.\n",
    "* Calculate the sum over all outcomes $<n> = \\sum_{n=N}^{\\infty} n \\cdot P_{n}$.\n",
    "\n",
    "\n",
    "I am enjoying writing up this problem because of the solving process I went through. It started out easy, then got harder and harder (as I tried to follow thinking from the Sixes problem), and then finally it got ridiculous. I then fell off Ridiculous Mountain with a small piece of the solution and after I looked at that for a few minutes I realized it was enough. In other words I had been dogmatically making the problem much harder than it was.\n",
    "\n",
    "\n",
    "### What is $pdf(n)$?\n",
    "\n",
    "After $n-1$ asks, the only way we can finish on ask $n$ is for there to be exactly $N-1$ acceptances so far. The number of ways of choosing $N-1$ people from $n-1$ people asked is $\\binom{n-1}{N-1}$.\n",
    "\n",
    "\n",
    "The probability that $N-1$ people all accept the offer is $a^{N-1}$. But all the remaining $n-1-(N-1)$ people must decline the offer, an *and* condition (so multiply), with probability $(1-a)^{n-1-(N-1)}$. \n",
    "\n",
    "\n",
    "So counting up all the ways of selecting $N-1$ lucky people from among $n-1$ and multiplying by the odds of those $N-1$ accepting and the remainder declining, we have the probability of reaching person $n$ with one job opening left. Then multiply that by $a$ to get the probability of finishing the recruitment process with this $n$th person. \n",
    "\n",
    "\n",
    "$$pdf(n) = a \\cdot \\binom{n-1}{N-1} \\cdot a^{N-1} \\cdot (1-a)^{n-N}$$\n",
    "\n",
    "\n",
    "\n",
    "### What is $<n>$?\n",
    "\n",
    "\n",
    "We immediately get the expectation for the number of people asked <math>\\;<n></math>:\n",
    "\n",
    "\n",
    "$$<n> \\; = \\; {\\left(\\frac{a}{1-a}\\right)}^{N}\\;\\sum_{n=N}^\\infty n \\cdot \\binom{n-1}{N-1} \\cdot {(1-a)}^{n}$$\n",
    "\n",
    "\n",
    "\n",
    "### What is $a$ given $k$?\n",
    "\n",
    "Before going further with this I should mention that I didn't go to the trouble of verifying that the pdf sums to $1$. Which it must if it is a good pdf (per the Sixes problem). Assuming this is ok then we can move on to the $k$ problem:\n",
    "\n",
    "\n",
    "Take a small number $\\epsilon$ and say that the probability of person $kN + 1$ being offered the job is less than that. Roughly speaking we can translate this to set the cumulative distribution function or cdf equal to $1 - \\epsilon$ for $n = k \\cdot N$. So here $k$ is just some multiplier greater than one to indicate the likely range of \"asked\" candidates from the ordered list. With $k$ and $N$ fixed we can proceed to solve for $a$, the probability that a person when asked will accept the job.\n",
    "\n",
    "\n",
    "$$1 - \\epsilon = {\\left(\\frac{a}{1-a}\\right)}^{N} \\cdot \\sum_{n=N}^{\\infty}\\binom{n-1}{N-1} \\cdot {(1-a)}^{n}$$\n",
    "\n",
    "\n",
    "\n",
    "Reducing this to a polynomial in $a$ looks pretty dreadful, let alone solving it, so either there are some nice ways to fix this up or it will only lend itself to a computer solution. One thought is to set $a$ to $1/2$ and pick small values of $N$ and $k$ to see what $\\epsilon$ looks like, or vice versa.\n",
    "\n",
    "\n",
    "* As with the Sixes problem: Need to write some code, choose a few examples, put in some charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bayesian stuff\n",
    "\n",
    "***I think a really good problem to try and solve (motivated once again at least in part by Knuth) is given a distribution of observed sum rolls for two dice: What are the numbers written on the dice?***\n",
    "\n",
    "Bayesian Inference and Data Analysis: A Bayesian Tutorial. \n",
    "\n",
    "The purpose here is to arrive at a method called '''''MCMC''''' from not first principles but at least a sketch of them.\n",
    "\n",
    "\n",
    "### MEX-EX and Bayes Theorem\n",
    "\n",
    "Let's make an observation '''A''' and suppose that this is somehow (perhaps?) causally connected to a set of driving conditions <math>B = {B_i}</math>. Furthermore suppose that those conditions are mutually exclusive and exhaustive, hence MEX-EX (my term). The connection between A and B is not deterministic but rather probabilistic. For example A might be the temperature of an outdoor thermometer and the B's might be seasons. This situation is the set-up for both the theorem of Bayes and for a system of inferring information in a probabilistic setting called ''Bayesian Inference''. And Bayesian Inference is carried to a broader set of ideas called Bayesian Analysis. \n",
    "\n",
    "\n",
    "First point: MEX-EX seems to be a (useful) analog to a pdf: The sum of <big><math>P(B_i)</math></big> must be 1. \n",
    "\n",
    "\n",
    "Second point: There is an implicit model relating dependent probabilities that works in both directions. Or it should eventually become an explicit model, case by case. One of the important ideas here is that the model itself is up for grabs: It might be good, it might be lousy; and we try and let the data tell us which within the Bayesian framework.\n",
    "\n",
    "\n",
    "So on to Bayes' Theorem.\n",
    "\n",
    "\n",
    "$$\n",
    "P(B_i|A) = \n",
    "\\frac{P(A|B_i)\\;\\cdot\\;P(B_i)}{\\sum_{j} P(A|B_j)\\;\\cdot\\;P(B_j)}\n",
    "$$\n",
    "\n",
    "\n",
    "This derives from the definition of the probability of both A and Bi occurring, written P(A, Bi). I won't bother formatting Bi to save time. Clearly P(A, Bi) = P(A|Bi) x P(Bi). The probability of the temperature being 5 deg C and it being winter is the probability of a 5 deg temperature when it is winter (estimated at 0.05) times the probability that it ''is'' winter (at an arbitrary date this is 0.25). So that gives me a probability of it being 5 deg C and winter as 1/80.\n",
    "\n",
    "\n",
    "I will say, by the way, that both temperature and season are ''states'' of the system (the authors say 'of nature'); but they stick with the idea that A is an observation and {Bi} are the MEX-EX states. I arbitrarily state that in winter the temperature is between -9 and +10 each with equal probability; so that is how I got to 1/20. But do I believe that the chances that it is winter and 5 deg are 1/80? Let's do the same with two dice. \n",
    "\n",
    "\n",
    "What are the chances of box cars? 1/36. P(6|6) = 1/6 and P(6) = 1/6 so that works for independent events. I think the idea is ok for now. So onward with a particular Bi:\n",
    "\n",
    "\n",
    "P(Bi, A) = P(A, Bi) is ok.\n",
    "\n",
    "\n",
    "So P(Bi|A)P(A) = P(A|Bi)P(Bi) is ok.\n",
    "\n",
    "\n",
    "So P(Bi|A) = P(A|Bi)P(Bi)/P(A) is ok.\n",
    "\n",
    "\n",
    "And then finally we say that the some of all conditional probabilities for A over the various B's is, in total, the probability of A since the B's are MEX-EX. That's only fair. So now that's Bayes' Theorem.   \n",
    "\n",
    "Inference follows: Your dependency model across all MEX-EX states is the characterization of reality that determines probabilities of independent states given your observation A. What is your model? It is a set of two probability distributions: P(A|Bi) and P(Bi) for all I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
