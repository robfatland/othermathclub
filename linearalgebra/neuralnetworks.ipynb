{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b0096b-3571-4ac2-95e2-b0237a4fd065",
   "metadata": {},
   "source": [
    "# Notes from the 3Blue1Brown YouTube series on neural networks\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "\n",
    "\n",
    "- [Grant Sanderson's first video in the Neural Net series](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "- [Steve Brunton's...]\n",
    "- [Michael Nielsen's book on Deep Learning and Neural Nets](http://neuralnetworksanddeeplearning.com/)\n",
    "    - [Michael Nielsen's MNIST OCR walkthrough](https://github.com/mnielsen/neural-networks-and-deep-learning)\n",
    "- [Gil Strang's course reference at MIT](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/)\n",
    "    - [Gil Strang: First in a CNN lecture series](https://www.youtube.com/watch?v=sx00s7nYmRM&pp=ygUaZ2lsIHN0cmFuZyBuZXVyYWwgbmV0d29ya3M%3D)\n",
    "- [Chris Olah's blog](http://colah.github.io/)\n",
    "- [Articles at *distill*](https://distill.pub/)\n",
    "- [Samson Zhang \"ground up from scratch no PyTorch\" neural net build (MNIST revisited) video](https://www.youtube.com/watch?v=w8yWXqWQYmU)\n",
    "\n",
    "\n",
    "## Fundamentals\n",
    "\n",
    "\n",
    "**Machine Learning** uses data to somehow determine how a model behaves.\n",
    "\n",
    "\n",
    "## Video 1: But what is a neural network?\n",
    "\n",
    "\n",
    "- The 28 x 28 pixel grid gives values called *activations*\n",
    "    - This is stacked to give the activation layer\n",
    "    - We end up not caring how the pixels are sorted into this layer\n",
    "        - This mapping intentionally does not try to retain the spatial information of the grid\n",
    "- The activation value for each pixel is real on [0, 1]\n",
    "- The second layer happens to have 16 neurons; 784 x 16 weights\n",
    "- There are also 16 bias values, one for each layer-2 neuron\n",
    "    - Bias acts like a threshold: It sets a bar to clear\n",
    "- The result is of the weighted sum plus bias is unconstrained...\n",
    "    - ...but we are interested in something constrained to be on (say) [0, 1]\n",
    "    - This is precisely analogous to a compressor in audio signal processing\n",
    "    - This compression was originally done with a sigmoid function, as in:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "More recently this has becomea a Rectifier Linear Unit funciton: $ReLU(a) = max(0, a)$.\n",
    "Pronounced \"Ray - Loo\".\n",
    "Gil Strand refers to this in one of his lectures. I will continue to indicate compression\n",
    "as $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff469b-a5b9-409c-99ed-338969d94a33",
   "metadata": {},
   "source": [
    "- sum of weights = 784 * 16 + 16 * 16 + 16 * 10\n",
    "- sum of biases = 16 + 16 + 10\n",
    "\n",
    "\n",
    "13k total parameters\n",
    "\n",
    "\n",
    "Exhortation: Dig into ***why?*** Challenge your assumptions.\n",
    "\n",
    "\n",
    "- Activations are a $1 \\times n$ column vector $A$\n",
    "- Weights are an $m \\times n$ matrix $W$\n",
    "- Bias is a $1 \\times m$ column vector\n",
    "- Second layer of neurons is a $1 \\times m$ column vector $N$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "{\n",
    "N = \\sigma( W \\cdot A + B )\n",
    "}\n",
    "\\end{align}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7909d52-d14c-47f7-843c-3898b0fc0561",
   "metadata": {},
   "source": [
    "## Video 2: Gradient descent and how neural networks learn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Introduces a training dataset: With answers!\n",
    "- MNIST database is freely available\n",
    "- Now we start in on *the calculus exercise*\n",
    "- Initial: Random values\n",
    "\n",
    "\n",
    "Now we are introduced to a cost function: \n",
    "\n",
    "\n",
    "- Go through the (presumably wrong) values in the output vector $a_i$.\n",
    "- For each: Difference the value from the correct value $0, 0, 0, 0, 1, 0, 0, 0, 0, 0$\n",
    "- Square this\n",
    "- Add them up\n",
    "- Higher cost value: The worse this set of weights performed\n",
    "- This is a result for but one example\n",
    "    - This produces a single number from the combination of 784 pixel values and 13k weights and biases\n",
    "\n",
    "\n",
    "Consider the average cost over the entire training set.\n",
    "\n",
    "\n",
    "Now drop into single-variable calculus thinking: Use the local slope to determine a step to take\n",
    "in searching for a minimum. Steeper slope: Bigger step. Small slope: Small step (avoid overshoot).\n",
    "\n",
    "\n",
    "Now move to cost as a surface above two variables (up from one). Now we are taking gradients.\n",
    "\n",
    "\n",
    "So what is moving to calculate the local surface gradient? Those two variables; suppose they are\n",
    "the first two weights $w_1$ and $w_2$. Now we have the steepest direction; and we use the negative\n",
    "of that (as it points *upward*) to descend to some minimum. But of course there is the danger \n",
    "that it is a *local* minimum.\n",
    "\n",
    "\n",
    "And now of course go from 2 dimensions to 13,002 dimensions of input.\n",
    "\n",
    "\n",
    "$- \\nabla C(\\vec{w})$\n",
    "\n",
    "\n",
    "Gradient calculation in this context is called *back propagation*: Next video.\n",
    "\n",
    "\n",
    "\n",
    "All we mean when we say a network is learning is that it is minimizing its cost function. \n",
    "\n",
    "\n",
    "\n",
    "So let's keep in mind: \n",
    "\n",
    "\n",
    "- We have multiple layers of neurons, weights, biases to start with\n",
    "- These represent a progression from 784 inputs to one result\n",
    "- And we have test data including *correct* answers\n",
    "- So we can determine cost function values across the entire test dataset\n",
    "- And this cost function $C$ has a mean value\n",
    "- And somehow we can calculate the gradient of $C$: A 13,000 element vector\n",
    "    - Whose elements indicate changes to apply; by both sign and magnitude\n",
    "    - ...so that we can start over with new weights; and iterate\n",
    "\n",
    "\n",
    "Then there is **the test**: Score a version of the network on data it has never seen before.\n",
    "\n",
    "\n",
    "\n",
    "### Does the network behave understandably?\n",
    "\n",
    "\n",
    "\n",
    "- No. \n",
    "- The weights from the 784 layer to the 16 layer (when viewed as 28 x 28 images) are just random-looking...\n",
    "- ...all 16 of them...\n",
    "- ...and will classify random junk as a particular digit with high confidence\n",
    "- ...and has no mechanism to actually *draw* an archetype 3\n",
    "\n",
    "\n",
    "Grant observes that there is no mechanism in the system for uncertainty. The result vectors\n",
    "for example are always certain: 9 zeros and a single 1.\n",
    "\n",
    "\n",
    "The video concludes with remarks on \"memorizing the dataset\" by means of all these parameters; \n",
    "what I suspect is called over-fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be14871-1fd6-4f33-a3de-c3a2d4159daf",
   "metadata": {},
   "source": [
    "## Video 3: Backpropagation: Plausible story\n",
    "\n",
    "\n",
    "- Backpropagation as a compelling story without notation or calculus\n",
    "- Consider weights and activations in the penultimate layer\n",
    "    - Proceeding from the weighted sum: Converse concept:\n",
    "        - \"Modify Weights where Activations are high for impact!\"\n",
    "        - \"Modify Activations where Weights are high for impact!\"\n",
    "    - Complexity concept:\n",
    "        - We are considering 10 (not just the correct +1 neuron) end activations\n",
    "        - Multiplexing means individual cases are given a vote, not final say\n",
    "\n",
    "\n",
    "We want to find a minimum, hence gradient descent. \n",
    "\n",
    "\n",
    "## Video 4: Backpropagation: Calculus basis\n",
    "\n",
    "\n",
    "\"What is SGD?\"\n",
    "\n",
    "\n",
    "- Backpropagation as calculus\n",
    "- Principle idea is multi-variate calculus chain rule\n",
    "- A given end-neuron (e.g. answer = 3) is impacted by an activation (from the prior neuron layer), a weight and a bias\n",
    "- The idea is to get the gradient of the cost function in terms of chain rule partial derivatives\n",
    "- Back-prop refers to chaining backwards up the network until you arrive at the stimuls activation layer. These can not be modified.\n",
    "- Also we can think in terms of modifying other feeder neuron activation layers...\n",
    "    - ...but this is actually expressed in terms of -- in turn -- that neuron's inbound weight and bias\n",
    "- The other dial here\n",
    "    - We calculate the cost function gradient for one input case\n",
    "    - ...but in fact we do some sort of average over many training inputs\n",
    "    - ...but not every single one as this becomes computationally prohibitive\n",
    "    - ...so we resort to random samples creating cohorts\n",
    "    - ...hence stochastic\n",
    "    - ...hence stochastic gradient descent\n",
    "    - ...hence SGD \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66124c4b-c06f-4d20-b093-0e0a21b1928a",
   "metadata": {},
   "source": [
    "## Video 5: GPT\n",
    "\n",
    "\n",
    "* Transformers are the next key idea\n",
    "    * Consist of alternating **Attention Blocks** and **Perceptron Blocks**\n",
    "    * Predict the next word by generating a pdf over a set of words\n",
    "* First we need encoding as tokens (words or fragments)\n",
    "* Then we need embedding: Each token is assigned a vector value\n",
    "    * The vector space is \"defined\" by the model\n",
    "    * Similar words (I use words instead of tokens as roughly equivalent)...\n",
    "        * ...wind up with roughly aligned vector values\n",
    "    * The processing step is now to iterate through Attention and Perceptron blocks\n",
    "        * The Attention block allows the series of vectors to interact with one another\n",
    "            * ...hence the vector elements are modified\n",
    "        * The Perceptron blocks operate on the vectors in parallel\n",
    "            * ...so no interactivity in this step\n",
    "            * \"Multi-layer Perceptrong\" or equivalently \"Feed Forward Layer\"\n",
    "            * What this does is, for now, a mystery\n",
    "                * ...but it is certainly more linear algebra\n",
    "        * There is also some normalization going on, in passing\n",
    "     \n",
    "The end result is understood as the now-thoroughly-modified last vector in\n",
    "the token sequence. And this modified vector is then used to generate the pdf,\n",
    "a distribution of probabilities for a set of possible \"next words\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c86ba6-6584-407b-8b73-484e727adbb7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
